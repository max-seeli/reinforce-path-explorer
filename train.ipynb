{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Reinforcement Learning Agent\n",
    "This notebook contains all the necessary code to train different policies and compare them to analyze the performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from map import MapLoader\n",
    "from monte_carlo import MonteCarlo, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(agent, start_pos = (0, 16)):\n",
    "    # Display the resulting policy for a subset of states for clarity\n",
    "    for state in agent.policy.policy.keys():\n",
    "        # Only show policy for zero velocity states for brevity\n",
    "        if state[2:] == (0, 0):  \n",
    "            print(f\"Policy at {state}: {agent.policy[state]}\")\n",
    "\n",
    "    # Display the number of states in the policy\n",
    "    print(f\"There are {len(agent.policy)} states in policy\")\n",
    "    \n",
    "    # Verify the policy by running an episodes\n",
    "    episode = agent.generate_episode(start=start_pos)\n",
    "    print(\"Episode:\")\n",
    "    for s, a, r in episode:\n",
    "        print(f\"State: {s}, Action: {a}, Reward: {r}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_1 = MapLoader.load_map(\"./maps/map1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:24<00:00,  3.78it/s]\n"
     ]
    }
   ],
   "source": [
    "mc_map = MonteCarlo(map_1, num_episodes=1_000, train_epsilon=0.9, policy_filename=\"policies/map_09.txt.policy\")\n",
    "mc_map.monte_carlo_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy at (6, 16, 0, 0): decrease_vy\n",
      "Policy at (4, 16, 0, 0): decrease_vy\n",
      "Policy at (8, 16, 0, 0): increase_vx\n",
      "Policy at (1, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (5, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (7, 16, 0, 0): decrease_vy\n",
      "Policy at (3, 16, 0, 0): decrease_vy\n",
      "Policy at (2, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (9, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (0, 16, 0, 0): increase_vx_decrease_vy\n",
      "There are 2397 states in policy\n",
      "Episode:\n",
      "State: (0, 16, 0, 0), Action: increase_vx_decrease_vy, Reward: -1\n",
      "State: (1, 15, 1, -1), Action: decrease_vx_vy, Reward: -1\n",
      "State: (1, 13, 0, -2), Action: increase_vx, Reward: -1\n",
      "State: (2, 11, 1, -2), Action: decrease_vx_increase_vy, Reward: -1\n",
      "State: (2, 10, 0, -1), Action: decrease_vx_vy, Reward: -1\n",
      "State: (1, 8, -1, -2), Action: increase_vx, Reward: -1\n",
      "State: (1, 6, 0, -2), Action: increase_vx_vy, Reward: -1\n",
      "State: (2, 5, 1, -1), Action: increase_vx, Reward: -1\n",
      "State: (4, 4, 2, -1), Action: increase_vy, Reward: 1\n"
     ]
    }
   ],
   "source": [
    "print_policy(mc_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.90it/s]\n"
     ]
    }
   ],
   "source": [
    "mc_map = MonteCarlo(map_1, num_episodes=1_000, train_epsilon=0.7, policy_filename=\"policies/map_07.txt.policy\")\n",
    "mc_map.monte_carlo_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy at (7, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (2, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (6, 16, 0, 0): decrease_vy\n",
      "Policy at (4, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (5, 16, 0, 0): decrease_vy\n",
      "Policy at (9, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (0, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (1, 16, 0, 0): decrease_vy\n",
      "Policy at (3, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (8, 16, 0, 0): decrease_vx_vy\n",
      "There are 2292 states in policy\n",
      "Episode:\n",
      "State: (0, 16, 0, 0), Action: increase_vx_decrease_vy, Reward: -1\n",
      "State: (1, 15, 1, -1), Action: decrease_vy, Reward: -1\n",
      "State: (2, 13, 1, -2), Action: decrease_vx, Reward: -1\n",
      "State: (2, 11, 0, -2), Action: no_change, Reward: -1\n",
      "State: (2, 9, 0, -2), Action: increase_vx, Reward: -1\n",
      "State: (3, 7, 1, -2), Action: decrease_vx, Reward: -1\n",
      "State: (3, 5, 0, -2), Action: increase_vx_vy, Reward: -1\n",
      "State: (4, 4, 1, -1), Action: increase_vx_vy, Reward: 1\n"
     ]
    }
   ],
   "source": [
    "print_policy(mc_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 65.99it/s]\n"
     ]
    }
   ],
   "source": [
    "mc_map = MonteCarlo(map_1, num_episodes=1_000, train_epsilon=0.5, policy_filename=\"policies/map_05.txt.policy\")\n",
    "mc_map.monte_carlo_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy at (9, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (2, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (3, 16, 0, 0): decrease_vy\n",
      "Policy at (7, 16, 0, 0): decrease_vy\n",
      "Policy at (6, 16, 0, 0): decrease_vy\n",
      "Policy at (0, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (4, 16, 0, 0): increase_vy\n",
      "Policy at (5, 16, 0, 0): increase_vx_decrease_vy\n",
      "Policy at (1, 16, 0, 0): decrease_vx_vy\n",
      "Policy at (8, 16, 0, 0): decrease_vx_vy\n",
      "There are 2178 states in policy\n",
      "Episode:\n",
      "State: (0, 16, 0, 0), Action: increase_vx_decrease_vy, Reward: -1\n",
      "State: (1, 15, 1, -1), Action: decrease_vx_vy, Reward: -1\n",
      "State: (1, 13, 0, -2), Action: no_change, Reward: -1\n",
      "State: (1, 11, 0, -2), Action: decrease_vx, Reward: -1\n",
      "State: (0, 9, -1, -2), Action: increase_vx, Reward: -1\n",
      "State: (0, 7, 0, -2), Action: increase_vx, Reward: -1\n",
      "State: (1, 5, 1, -2), Action: increase_vx_vy, Reward: -1\n",
      "State: (3, 4, 2, -1), Action: decrease_vx_increase_vy, Reward: -1\n",
      "State: (4, 4, 1, 0), Action: increase_vx, Reward: 1\n"
     ]
    }
   ],
   "source": [
    "print_policy(mc_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
